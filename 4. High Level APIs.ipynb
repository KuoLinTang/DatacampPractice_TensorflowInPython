{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673520f0",
   "metadata": {},
   "source": [
    "# Defining neural networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e4228f",
   "metadata": {},
   "source": [
    "前幾章有提到利用 tensorflow 來建立 linear regression models 和 neural networks 並且示範了 1. high level (既有的 layer 函數) 2. low level (linear algebra) 兩種方法。Tensorflow 的方法都要一層一層的建，而且還要 specify 前一層的名稱，比較麻煩。<br/>\n",
    "這裡會介紹 Keras 這個 high level 的 API，可以更方便的建立模型並且避免複雜的迴圈過程。不過 high level 就代表著 flexibility 比較低，比較不能根據不同的狀況來隨意改變模型。\n",
    "\n",
    "---\n",
    "\n",
    "這一章主要是要將下圖中 (28 * 28) 的四個手勢建模，使得模型可以將新的照片分類。(4 個 output nodes，可用 softmax 這個 function 當作 output layer 的 activation function)\n",
    "\n",
    "![](Image/Image19.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850829e3",
   "metadata": {},
   "source": [
    "## Sequential API\n",
    "\n",
    "Sequential API 是 Keras 裡面的一個建立 neural networks 的樣板。這個樣板確立了模型擁有 input layer, hidden layers 和 output layer。稱為 sequential 的原因是因為每一個 layer 都是按照順序建立的，因此不用像是用 tensorflow 建模一樣去 specify 前一層是哪一個。\n",
    "\n",
    "---\n",
    "\n",
    "### 實際操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbb094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# define a sequential model\n",
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6633dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the first layer\n",
    "model.add(keras.layers.Dense(16, activation = \"relu\", input_shape = (28 * 28,)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f10c3d",
   "metadata": {},
   "source": [
    "input shape 是一個存有資料 dimension 的 tuple，由於我們每一張圖都是 28 * 28 的解析度，而且要 reshape 成 vector (1-dimensional tensor) 才能訓練模型，因此會被 reshape 成 28 * 28 個 element 的 vector。\n",
    "\n",
    "第一層必須 specify 輸入的 shape，才能讓電腦知道該如何建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e84ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the second layer\n",
    "model.add(keras.layers.Dense(8, activation = \"relu\"))\n",
    "\n",
    "# define the output layer\n",
    "model.add(keras.layers.Dense(4, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62bffad",
   "metadata": {},
   "source": [
    "除了第一層以外，其他層不需要指出 input shape，因為 sequential 樣板的關係 (順序)，後面幾層都知道前一層的 shape。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfce87a",
   "metadata": {},
   "source": [
    "**建立 optimizer 和 指定 loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd52cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77596f",
   "metadata": {},
   "source": [
    "**檢查模型各層的內容**\n",
    "可以用 model.summary() 來看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57fdfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 12,732\n",
      "Trainable params: 12,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896ca26",
   "metadata": {},
   "source": [
    "12560 = 28 * 28 * 16 + 16 (input layer 有 28 * 28 個 nodes，第一層有 16 個 nodes，因此總共有 28 * 28 * 16 個 weights。同時，第一層的每個 nodes 都有一個 bias，總共 16 個 bias)\n",
    "\n",
    "136 = 16 * 8 + 8\n",
    "\n",
    "36 = 8 * 4 + 4\n",
    "\n",
    "**此模型已經可以帶入資料訓練了。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e49dd",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n",
    "假設我的輸出是來自多個模型，如下圖：\n",
    "\n",
    "![](Image/Image20.jpg)\n",
    "\n",
    "則不能用 sequential API 而是用 functional API。\n",
    "\n",
    "---\n",
    "\n",
    "### 示範\n",
    "假設除了 28 * 28 的照片以外，還有 10 個額外的 features 也要加入 training data，因此就有兩大組 input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3dabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "\n",
    "# first input\n",
    "model1_inputs = tf.keras.Input(shape = (28*28,))\n",
    "\n",
    "# second input\n",
    "model2_inputs = tf.keras.Input(shape = (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae7db70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the first layer for the model 1\n",
    "model1_layer1 = tf.keras.layers.Dense(12, activation = \"relu\")(model1_inputs)\n",
    "\n",
    "# define the second layer for the model 1\n",
    "model1_layer2 = tf.keras.layers.Dense(4, activation = \"softmax\")(model1_layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f65e0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the first layer for the model 2\n",
    "model2_layer1 = tf.keras.layers.Dense(8, activation = \"relu\")(model2_inputs)\n",
    "\n",
    "# define the second layer for the model 2\n",
    "model2_layer2 = tf.keras.layers.Dense(4, activation = \"softmax\")(model2_layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d5cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two outputs\n",
    "merged = tf.keras.layers.add([model1_layer2, model2_layer2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de04b3",
   "metadata": {},
   "source": [
    "最後，定義一個合併的 functional model，並指定好 inputs 和 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166e18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs = [model1_inputs, model2_inputs], outputs = merged)\n",
    "\n",
    "# compile the model\n",
    "model.compile(\"adam\", loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da7eb519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 12)           9420        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 8)            88          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            52          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            36          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4)            0           dense_4[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,596\n",
      "Trainable params: 9,596\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225297cb",
   "metadata": {},
   "source": [
    "**此模型已經可以帶入資料訓練了。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581ce48",
   "metadata": {},
   "source": [
    "# Training and validation (evaluation) with Keras\n",
    "\n",
    "過程： <br/>\n",
    "1. Load and clean data (前幾章已教過)\n",
    "2. Define model (前幾章已教過)\n",
    "3. Train and validate model\n",
    "4. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0e060",
   "metadata": {},
   "source": [
    "## Training model\n",
    "利用 model.fit(features, labels) 這個函數來訓練\n",
    "\n",
    "必要的參數： features, labels ，代表 training data 的 independent variables 和 dependent variables\n",
    "\n",
    "非必要的參數： batch_size, epochs, validation_split\n",
    "\n",
    "1. Batch_size： 代表了一個 batch 包含幾筆資料。當 batch_size 很大時，要將很多資料同時放入記憶體中，在某些記憶體較小的電腦可能會跑不動。而每個 batch 跑完就會更新 weights 和 bias 等參數，因此當 batch_size 太小時，資料不足使得參數的改變不一定合理。(default = 32)\n",
    "\n",
    "2. epochs： 代表總共要跑幾次 epochs (epoch 代表完整的跑完一次所有的 batches)。當 epochs 很大時，代表經過較多的訓練 iterations，可能比較接近 global minima，但也代表訓練時間較久。\n",
    "\n",
    "3. validation_split： 這裡要傳入一個屆於 1~0 的數，代表分給 validation set 的比例。這樣模型訓練時就會將原始資料按照比例分成 training set 和 validation set。在每一個 epoch 後都可以看到模型在 training set 和 validation set 的個別表現 (如果 training set 的 loss 比 validation set 的 loss 還小，則有 overfitting 的現象)\n",
    "\n",
    "![](Image/Image21.jpg)\n",
    "\n",
    "當訓練過程中看到 overfitting 的跡象，就要停止訓練，並在模型中加入一些 regularisation 的方法，例如 dropout。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "model.fit(FEATURES, LABELS)\n",
    "\n",
    "# Evaluate (利用一開始就分出來的 testing data)\n",
    "model.evaluate(TESTING_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa47bd",
   "metadata": {},
   "source": [
    "## 實際操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1745fb2",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbf98b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
      "0    1  142  143  146  148  149  149  149  150  151  ...    0   15   55   63   \n",
      "1    0  141  142  144  145  147  149  150  151  152  ...  173  179  179  180   \n",
      "2    1  156  157  160  162  164  166  169  171  171  ...  181  197  195  193   \n",
      "3    3   63   26   65   86   97  106  117  123  128  ...  175  179  180  182   \n",
      "4    1  156  160  164  168  172  175  178  180  182  ...  108  107  106  110   \n",
      "\n",
      "   779  780  781  782  783  784  \n",
      "0   37   61   77   65   38   23  \n",
      "1  181  181  182  182  183  183  \n",
      "2  193  191  192  198  193  182  \n",
      "3  183  183  184  185  185  185  \n",
      "4  111  108  108  102   84   70  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "=========================================================\n",
      "[[142 143 146 ...  65  38  23]\n",
      " [141 142 144 ... 182 183 183]\n",
      " [156 157 160 ... 198 193 182]\n",
      " ...\n",
      " [161 164 166 ... 240 240 240]\n",
      " [162 164 167 ... 166 176 170]\n",
      " [145 148 150 ... 173 168 159]]\n",
      "(2000, 784)\n",
      "<class 'numpy.ndarray'>\n",
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "(2000, 4)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv(\"Datasets/slmnist.csv\", header = None)\n",
    "print(data.head())\n",
    "\n",
    "print(\"=========================================================\")\n",
    "\n",
    "# transform dataframe into numpy array\n",
    "features_array = np.array(data.drop(labels = 0, axis = 1), dtype = np.float32)\n",
    "print(features[0:10, :])\n",
    "print(features.shape)\n",
    "print(type(features))\n",
    "\n",
    "# one-hot encoding\n",
    "labels = pd.get_dummies(data.iloc[:,0], prefix='Class')\n",
    "\n",
    "# transform labels into numpy array\n",
    "labels_array = np.array(labels, dtype = np.float32)\n",
    "print(labels_array[0:10, :])\n",
    "print(labels_array.shape)\n",
    "print(type(labels_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad31bcd",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9e4ef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 12,732\n",
      "Trainable params: 12,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# first layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape = (784,)))\n",
    "\n",
    "# second layer\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# define optimizer and loss function\n",
    "model.compile('adam', loss = 'categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "\n",
    "# summarise the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059f619",
   "metadata": {},
   "source": [
    "### Train and validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a22b435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 8.8615 - accuracy: 0.2650 - val_loss: 1.3869 - val_accuracy: 0.2500\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.3864 - accuracy: 0.2500 - val_loss: 1.3870 - val_accuracy: 0.2500\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.3864 - accuracy: 0.2500 - val_loss: 1.3871 - val_accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.3864 - accuracy: 0.2500 - val_loss: 1.3871 - val_accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.3864 - accuracy: 0.2483 - val_loss: 1.3872 - val_accuracy: 0.2150\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.3863 - accuracy: 0.2422 - val_loss: 1.3871 - val_accuracy: 0.2150\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 953us/step - loss: 1.3864 - accuracy: 0.2289 - val_loss: 1.3871 - val_accuracy: 0.2150\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.3863 - accuracy: 0.2539 - val_loss: 1.3871 - val_accuracy: 0.2150\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.3864 - accuracy: 0.2539 - val_loss: 1.3874 - val_accuracy: 0.2150\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.3864 - accuracy: 0.2539 - val_loss: 1.3875 - val_accuracy: 0.2150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24080dad550>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features_array, labels_array, batch_size = 32, epochs = 10, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd4bdb",
   "metadata": {},
   "source": [
    "## 嘗試另一個模型 (更低的 learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83aefd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 8)                 8200      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 812,076\n",
      "Trainable params: 812,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "\n",
    "# first layer\n",
    "model2.add(tf.keras.layers.Dense(1024, activation='relu', input_shape = (784,)))\n",
    "\n",
    "# second layer\n",
    "model2.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model2.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# define optimizer and loss function\n",
    "model2.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "\n",
    "# summarise the model\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b180cba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3913 - val_accuracy: 0.2320\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3913 - val_accuracy: 0.2320\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3915 - val_accuracy: 0.2320\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3847 - accuracy: 0.2680 - val_loss: 1.3914 - val_accuracy: 0.2320\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3847 - accuracy: 0.2680 - val_loss: 1.3916 - val_accuracy: 0.2320\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3917 - val_accuracy: 0.2320\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3916 - val_accuracy: 0.2320\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3916 - val_accuracy: 0.2320\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3914 - val_accuracy: 0.2320\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3915 - val_accuracy: 0.2320\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3914 - val_accuracy: 0.2320\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3847 - accuracy: 0.2680 - val_loss: 1.3916 - val_accuracy: 0.2320\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3915 - val_accuracy: 0.2320\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3913 - val_accuracy: 0.2320\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3914 - val_accuracy: 0.2320\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3912 - val_accuracy: 0.2320\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3913 - val_accuracy: 0.2320\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3915 - val_accuracy: 0.2320\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3913 - val_accuracy: 0.2320\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3847 - accuracy: 0.2680 - val_loss: 1.3915 - val_accuracy: 0.2320\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.3845 - accuracy: 0.2680 - val_loss: 1.3914 - val_accuracy: 0.2320\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3912 - val_accuracy: 0.2320\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 1.3848 - accuracy: 0.2680 - val_loss: 1.3908 - val_accuracy: 0.2320\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3912 - val_accuracy: 0.2320\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3912 - val_accuracy: 0.2320\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3913 - val_accuracy: 0.2320\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3913 - val_accuracy: 0.2320\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3914 - val_accuracy: 0.2320\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3913 - val_accuracy: 0.2320\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.3846 - accuracy: 0.2680 - val_loss: 1.3914 - val_accuracy: 0.2320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x240813a4fa0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(features_array, labels_array, batch_size = 32, epochs = 30, validation_split = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1278d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26e14510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 608us/step - loss: 1.3864 - accuracy: 0.2500\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3880 - accuracy: 0.2500\n",
      "Model 1: Accuracy - 0.25\n",
      "Model 2: Accuracy - 0.25\n"
     ]
    }
   ],
   "source": [
    "model1_acc = model.evaluate(features_array, labels_array)\n",
    "model2_acc = model2.evaluate(features_array, labels_array)\n",
    "\n",
    "print(\"Model 1: Accuracy - {}\".format(model1_acc[1]))\n",
    "print(\"Model 2: Accuracy - {}\".format(model2_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f438c4c",
   "metadata": {},
   "source": [
    "# Training models with the Estimators API\n",
    "![](Image/Image22.jpg)\n",
    "\n",
    "The Estimator API 是一個 high level tensorflow submodule (less flexible)。由於是最頂層的 API，因此不能隨便調整架構，但使用 (deploy) 的時間最短，可快速運用，也可以寫少一點 code。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b78ea",
   "metadata": {},
   "source": [
    "## Training process\n",
    "\n",
    "1. Define feature columns (specify shape and type of data)\n",
    "2. Load and transform data within a function (定義一個函數，此函數要輸出一個 dictionary 物件，包含 features 和 labels)\n",
    "3. Define custom estimators with different architectures (也可以用 premade estimators)\n",
    "4. Apply train operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8007535",
   "metadata": {},
   "source": [
    "### 1. Define feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc22be7d",
   "metadata": {},
   "source": [
    "**以前一章節的資料為例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "30899ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "\n",
    "# define a numeric feature column\n",
    "size = tf.feature_column.numeric_column(\"size\")\n",
    "\n",
    "# define a categorical feature column\n",
    "rooms = tf.feature_column.categorical_column_with_vocabulary_list(\"room\", [\"1\",\"2\",\"3\",\"4\",\"5\"])\n",
    "\n",
    "# merge the two features\n",
    "features_list = [size, rooms]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6468cfa",
   "metadata": {},
   "source": [
    "**以圖像辨識的資料為例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05d09978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import tensorflow as tf\n",
    "\n",
    "# define a matrix feature column\n",
    "image = tf.feature_column.numeric_column(\"image\", shape = (784,))\n",
    "\n",
    "# transform into list\n",
    "features_list = [image]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a3901",
   "metadata": {},
   "source": [
    "### 2. Load and transform data within a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0f298",
   "metadata": {},
   "source": [
    "**以前一章節的資料為例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a3862d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to load and transform data (只以三筆資料為例)\n",
    "def input_function():\n",
    "    # define feature dictionary\n",
    "    features = {\"size\": [1340, 1690, 2720], \"room\": [1, 3, 4]}\n",
    "    # define labels\n",
    "    labels = [221900, 538000, 180000]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bbb72",
   "metadata": {},
   "source": [
    "### 3. Define custom estimators with different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbb30b",
   "metadata": {},
   "source": [
    "**以前一章節的資料為例** (Regression model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f4f820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\TANGKU~1\\AppData\\Local\\Temp\\tmpo16ggdvi\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\TANGKU~1\\\\AppData\\\\Local\\\\Temp\\\\tmpo16ggdvi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From C:\\Users\\TangKuoLin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Items of feature_columns must be a <class 'tensorflow.python.feature_column.feature_column_v2.DenseColumn'>. You can wrap a categorical column with an embedding_column or indicator_column. Given: VocabularyListCategoricalColumn(key='room', vocabulary_list=('1', '2', '3', '4', '5'), dtype=tf.string, default_value=-1, num_oov_buckets=0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-5c5bcf1fc75f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# train the regression model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1202\u001b[0m           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\n\u001b[0;32m   1203\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n\u001b[0m\u001b[0;32m   1205\u001b[0m                                            self.config)\n\u001b[0;32m   1206\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[1;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m_model_fn\u001b[1;34m(features, labels, mode, config)\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m       \u001b[1;34m\"\"\"Call the defined shared dnn_model_fn_v2.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1159\u001b[1;33m       return dnn_model_fn_v2(\n\u001b[0m\u001b[0;32m   1160\u001b[0m           \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m           \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36mdnn_model_fn_v2\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    554\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m   logits, trainable_variables, update_ops = _dnn_model_fn_builder_v2(\n\u001b[0m\u001b[0;32m    557\u001b[0m       \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits_dimension\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m       \u001b[0mhidden_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m_dnn_model_fn_builder_v2\u001b[1;34m(units, hidden_units, feature_columns, activation_fn, dropout, batch_norm, features, mode)\u001b[0m\n\u001b[0;32m    490\u001b[0m     raise ValueError('units must be an int.  Given type: {}'.format(\n\u001b[0;32m    491\u001b[0m         type(units)))\n\u001b[1;32m--> 492\u001b[1;33m   dnn_model = _DNNModelV2(\n\u001b[0m\u001b[0;32m    493\u001b[0m       \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m       \u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, hidden_units, feature_columns, activation_fn, dropout, batch_norm, name, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mlayer_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_feature_column_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'input_layer'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mfeature_column_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feature_column_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         self._input_layer = tf.compat.v2.keras.layers.DenseFeatures(\n\u001b[0m\u001b[0;32m    290\u001b[0m             feature_columns=feature_columns, name=layer_name)\n\u001b[0;32m    291\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\feature_column\\dense_features_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, feature_columns, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDenseColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     super(DenseFeatures, self).__init__(\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\feature_column\\dense_features.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, feature_columns, trainable, name, partitioner, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDenseColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m     super(DenseFeatures, self).__init__(\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\feature_column\\base_feature_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, feature_columns, expected_column_type, trainable, name, partitioner, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feature_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_column_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[1;34m'Items of feature_columns must be a {}. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;34m'You can wrap a categorical column with an '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Items of feature_columns must be a <class 'tensorflow.python.feature_column.feature_column_v2.DenseColumn'>. You can wrap a categorical column with an embedding_column or indicator_column. Given: VocabularyListCategoricalColumn(key='room', vocabulary_list=('1', '2', '3', '4', '5'), dtype=tf.string, default_value=-1, num_oov_buckets=0)"
     ]
    }
   ],
   "source": [
    "# define a deep neural network regression\n",
    "model0 = tf.estimator.DNNRegressor(feature_columns = features_list, hidden_units = [10, 6, 6, 3])\n",
    "\n",
    "# train the regression model\n",
    "model0.train(input_function, steps = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a0f14",
   "metadata": {},
   "source": [
    "**以前一章節的資料為例** (Classification model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35746062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\TANGKU~1\\AppData\\Local\\Temp\\tmpkb192z12\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\TANGKU~1\\\\AppData\\\\Local\\\\Temp\\\\tmpkb192z12', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Items of feature_columns must be a <class 'tensorflow.python.feature_column.feature_column_v2.DenseColumn'>. You can wrap a categorical column with an embedding_column or indicator_column. Given: VocabularyListCategoricalColumn(key='room', vocabulary_list=('1', '2', '3', '4', '5'), dtype=tf.string, default_value=-1, num_oov_buckets=0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-8d13eee1b197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# train the regression model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1202\u001b[0m           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\n\u001b[0;32m   1203\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n\u001b[0m\u001b[0;32m   1205\u001b[0m                                            self.config)\n\u001b[0;32m   1206\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[1;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m_model_fn\u001b[1;34m(features, labels, mode, config)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[1;34m\"\"\"Call the defined shared dnn_model_fn_v2.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m       return dnn_model_fn_v2(\n\u001b[0m\u001b[0;32m    747\u001b[0m           \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m           \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36mdnn_model_fn_v2\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    554\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m   logits, trainable_variables, update_ops = _dnn_model_fn_builder_v2(\n\u001b[0m\u001b[0;32m    557\u001b[0m       \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits_dimension\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m       \u001b[0mhidden_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m_dnn_model_fn_builder_v2\u001b[1;34m(units, hidden_units, feature_columns, activation_fn, dropout, batch_norm, features, mode)\u001b[0m\n\u001b[0;32m    490\u001b[0m     raise ValueError('units must be an int.  Given type: {}'.format(\n\u001b[0;32m    491\u001b[0m         type(units)))\n\u001b[1;32m--> 492\u001b[1;33m   dnn_model = _DNNModelV2(\n\u001b[0m\u001b[0;32m    493\u001b[0m       \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m       \u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, hidden_units, feature_columns, activation_fn, dropout, batch_norm, name, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mlayer_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_feature_column_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'input_layer'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mfeature_column_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feature_column_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         self._input_layer = tf.compat.v2.keras.layers.DenseFeatures(\n\u001b[0m\u001b[0;32m    290\u001b[0m             feature_columns=feature_columns, name=layer_name)\n\u001b[0;32m    291\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\feature_column\\dense_features_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, feature_columns, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDenseColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     super(DenseFeatures, self).__init__(\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\feature_column\\dense_features.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, feature_columns, trainable, name, partitioner, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDenseColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m     super(DenseFeatures, self).__init__(\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\feature_column\\base_feature_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, feature_columns, expected_column_type, trainable, name, partitioner, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feature_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_column_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[1;34m'Items of feature_columns must be a {}. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;34m'You can wrap a categorical column with an '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Items of feature_columns must be a <class 'tensorflow.python.feature_column.feature_column_v2.DenseColumn'>. You can wrap a categorical column with an embedding_column or indicator_column. Given: VocabularyListCategoricalColumn(key='room', vocabulary_list=('1', '2', '3', '4', '5'), dtype=tf.string, default_value=-1, num_oov_buckets=0)"
     ]
    }
   ],
   "source": [
    "# define a deep neural network classifier\n",
    "model1 = tf.estimator.DNNClassifier(feature_columns = features_list, hidden_units = [32, 16, 8], n_classes = 4)\n",
    "\n",
    "# train the regression model\n",
    "model1.train(input_function, steps = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242662f",
   "metadata": {},
   "source": [
    "### 詳細教學\n",
    "https://www.tensorflow.org/guide/estimators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
